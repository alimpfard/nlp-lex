# toplevel syntax
#   constant  :- symbol :- string-literal
#   rule      :- symbol :: defn
#   normalise :- char <= char-list (comma separated? ask for ideas)
#   stopwords :- stopword quoted-string-literal (commas optional)
#   option    :- option _name_ _value_
#   -"string" subst with file content
#
# split with a default delimiter?
# NFA output + decompile back
#
# --

option lemmatise on

u0ffe  :- "\u0ffe"
number :: \d+
float  :: {{number}}\.{{number}}
word   :: [\w{{u0ffe}}]+

t      <= cdf
x      <= xyz

stopword "test", "shit", "blah", -"stopwords.list"

# TODO items
# - optimise regexp itself
#   - if all branches of an Alternative are literals, transform to character class { 1|2|3|4|5|6... -> [123456] }
#
#
# - Figure out how to handle:
#   - rule-based token operations (for common mistakes)
#       - word:a word:b -> word:ab
#       - word:a number:b -> wordnum:ba
#   - sentence boundary detection and optional impl of Viterbi with comptime generated model
#       - provide an implementation (https://github.com/alimpfard/citron-tp-test/tree/master/part-6) of HMM and Viterbi
#   - how to handle lemmatisation on different languages
#       - provide a DSL for writing lemmatisers?
#       - provide a few well-known implementations for specific languages?
#     - allow the user to provide her own implementation that generates llvm bytecode?
#         - calling convention?
#   - json in-out
#   - create python wrapper with compile-on-need
#   - 'option farsi on' to appease the gods
